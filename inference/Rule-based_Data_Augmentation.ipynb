{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csc_matrix,find,coo_matrix\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables ###\n",
    "\n",
    "# the path of raw rule files generated by AMIE 3\n",
    "data_path_list = [\n",
    "    './rules_file/rules_0.txt',\n",
    "    './rules_file/rules_1.txt',\n",
    "    './rules_file/rules_2.txt',\n",
    "    './rules_file/rules_3.txt',\n",
    "    './rules_file/rules_4.txt'\n",
    "]\n",
    "\n",
    "# path\n",
    "train_hrt_data_path = './ogb/wikikg90m_kddcup2021/processed/train_hrt.npy'\n",
    "relation_matrix_path = './rel_ht_spmat/'\n",
    "\n",
    "# the attribute of rule list\n",
    "V_Rule = 0\n",
    "V_Head_Coverage = 1\n",
    "V_Std_Confidence =2\n",
    "V_PCA_Confidence = 3\n",
    "V_Positive_Examples = 4\n",
    "V_Body_size = 5\n",
    "V_PCA_Body_size = 6\n",
    "V_Functional_variable = 7\n",
    "\n",
    "# other variables\n",
    "INV_NUM = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions about rules ###\n",
    "\n",
    "# get rules from the raw files\n",
    "def get_rules_from_file(rule_file_path):\n",
    "    fp = open(rule_file_path)\n",
    "    rule_num = -1\n",
    "    rules_lists = []\n",
    "    for line in fp.readlines():\n",
    "        if rule_num == -1:\n",
    "            items = line.strip().split('\\t')\n",
    "        else:\n",
    "            rule_list = line.strip().split('\\t')\n",
    "            rule_list[1] = float(rule_list[1]) # Head Coverage\n",
    "            rule_list[2] = float(rule_list[2]) # Std Confidence\n",
    "            rule_list[3] = float(rule_list[3]) # PCA Confidence\n",
    "            rule_list[4] = int(rule_list[4]) # Positive Examples\n",
    "            rule_list[5] = int(rule_list[5]) # Body size\n",
    "            rule_list[6] = int(rule_list[6]) # PCA Body size\n",
    "\n",
    "            rules_lists.append(rule_list)\n",
    "        rule_num += 1\n",
    "        \n",
    "    for i in range(rule_num):\n",
    "        rules_lists[i][0] = parse_rules(rules_lists[i][0])\n",
    "    return rules_lists\n",
    "\n",
    "\n",
    "# get the number of positive and negative examples\n",
    "def get_rule_PCA_examples(rules_lists):\n",
    "    rules_dict = {}\n",
    "    for i in range(len(rules_lists)):\n",
    "        pos_example = rules_lists[i][V_Positive_Examples]\n",
    "        neg_example = round( (pos_example / rules_lists[i][V_PCA_Confidence]) - pos_example )\n",
    "        rule = rules_lists[i][V_Rule]\n",
    "        if rule not in rules_dict.keys():\n",
    "            rules_dict[rule] = {}\n",
    "            rules_dict[rule]['NEG'] = neg_example\n",
    "            rules_dict[rule]['POS'] = pos_example\n",
    "        else:\n",
    "            rules_dict[rule]['NEG'] += neg_example\n",
    "            rules_dict[rule]['POS'] += pos_example\n",
    "    return rules_dict\n",
    "\n",
    "\n",
    "# convert dict to list \n",
    "# the element of rules_dict is in the form: { RULE: {NEG，NEG} }\n",
    "# the element of rules_lists is in the form: [RULE, POS, NEG, PAC_CONFIDENCE]\n",
    "def get_rule_list_from_dict(rules_dict):\n",
    "    rules_lists = []\n",
    "    \n",
    "    for rule in rules_dict.keys():\n",
    "        pos_example = rules_dict[rule]['POS']\n",
    "        neg_example = rules_dict[rule]['NEG']\n",
    "        pca_confidence = pos_example / (pos_example + neg_example)\n",
    "        rules_lists.append([rule, pos_example, neg_example, pca_confidence])\n",
    "    return rules_lists\n",
    "\n",
    "\n",
    "# parse the raw rule files\n",
    "def parse_rules(rule_str):\n",
    "    \n",
    "    rule_str_list = rule_str.split()\n",
    "    equal_index = rule_str_list.index('=>')\n",
    "    if equal_index // 3 == 1:\n",
    "        body_s1 = rule_str_list[0]\n",
    "        body_s2 = rule_str_list[2]\n",
    "        body_rel = int(rule_str_list[1])\n",
    "        \n",
    "        head_s1 = rule_str_list[4]\n",
    "        head_s2 = rule_str_list[6]\n",
    "        head_rel = int(rule_str_list[5])\n",
    "        \n",
    "        if head_s1 == body_s1 and head_s2 == body_s2:\n",
    "            return (body_rel, head_rel)\n",
    "        elif head_s1 == body_s2 and head_s2 == body_s1:\n",
    "            return (body_rel + INV_NUM, head_rel)\n",
    "        else:\n",
    "            return None\n",
    "    elif equal_index // 3 == 2:\n",
    "        body1_s1 = rule_str_list[0]\n",
    "        body1_s2 = rule_str_list[2]\n",
    "        body1_rel = int(rule_str_list[1])\n",
    "        \n",
    "        body2_s1 = rule_str_list[3]\n",
    "        body2_s2 = rule_str_list[5]\n",
    "        body2_rel = int(rule_str_list[4])\n",
    "        \n",
    "        head_s1 = rule_str_list[7]\n",
    "        head_s2 = rule_str_list[9]\n",
    "        head_rel = int(rule_str_list[8])\n",
    "        \n",
    "        if body1_s2 == body2_s1: \n",
    "            if body1_s1 == head_s1 and body2_s2 == head_s2: # (a r_1 b) (b r_2 c) => (a r_3 c) \n",
    "                return (body1_rel, body2_rel, head_rel) # (a r_1 b) (b r_2 c) => (a r_3 c) \n",
    "            elif body1_s1 == head_s2 and body2_s2 == head_s1: # (c r_1 b) (b r_2 a) => (a r_3 c) \n",
    "                return (body2_rel + INV_NUM, body1_rel+ INV_NUM, head_rel)  \n",
    "            else:\n",
    "                return None\n",
    "        elif body1_s1 == body2_s1: \n",
    "            if body1_s2 == head_s1 and body2_s2 == head_s2: # (a r_1 b) (a r_2 c) => (b r_3 c) \n",
    "                return (body1_rel + INV_NUM, body2_rel, head_rel) \n",
    "            elif body1_s2 == head_s2 and body2_s2 == head_s1: # (a r_1 b) (a r_2 c) => (c r_3 b) \n",
    "                return (body2_rel + INV_NUM, body1_rel, head_rel)  \n",
    "            else:\n",
    "                return None\n",
    "        elif body1_s1 == body2_s2: \n",
    "            if body1_s2 == head_s1 and body2_s1 == head_s2: # (a r_1 b) (c r_2 a) => (b r_3 c) \n",
    "                return (body1_rel + INV_NUM, body2_rel + INV_NUM, head_rel) \n",
    "            elif body1_s2 == head_s2 and body2_s1 == head_s1: # (a r_1 c) (b r_2 a) => (b r_3 c) \n",
    "                return (body2_rel, body1_rel, head_rel)  \n",
    "            else:\n",
    "                return None\n",
    "        elif body1_s2 == body2_s2: \n",
    "            if body1_s1 == head_s1 and body2_s1 == head_s2: # (b r_1 a) (c r_2 a) => (b r_3 c) \n",
    "                return (body1_rel, body2_rel + INV_NUM, head_rel) \n",
    "            elif body1_s1 == head_s2 and body2_s1 == head_s1: # (b r_1 a) (c r_2 a) => (c r_3 b) \n",
    "                return (body2_rel, body1_rel + INV_NUM, head_rel)  \n",
    "            else:\n",
    "                return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# load the relation matrix by id\n",
    "def load_spmat_by_id(rel):\n",
    "    return sparse.load_npz(relation_matrix_path + 'rel_ht_spmat_'+str(rel%INV_NUM)+'.npz')\n",
    "\n",
    "# make predictions by using rules，input format: (r_1, r_2, r_head) or (r_1, r_head)\n",
    "def get_new_triples_by_rule_scipy(rule):\n",
    "    if len(rule) == 2:\n",
    "        r1 = rule[0]\n",
    "        rh = rule[1]\n",
    "        \n",
    "        sp_r1 = load_spmat_by_id(r1)\n",
    "        sp_rh = load_spmat_by_id(rh)\n",
    "        \n",
    "        # for the inverse relations\n",
    "        if r1 >= INV_NUM:\n",
    "            sp_r1 = sp_r1.T\n",
    "        \n",
    "        sp_rule_head = sp_r1\n",
    "        sp_new_head = sp_rule_head - coo_matrix.multiply(sp_rule_head, sp_rh)\n",
    "        sp_new_head = sp_new_head.tocoo()\n",
    "        \n",
    "        new_triples = []\n",
    "        for i in range(len(sp_new_head.row)):\n",
    "            new_triples.append((sp_new_head.row[i], rh, sp_new_head.col[i]) )\n",
    "            \n",
    "        return new_triples\n",
    "    elif len(rule) == 3:\n",
    "        r1 = rule[0]\n",
    "        r2 = rule[1]\n",
    "        rh = rule[2]\n",
    "        \n",
    "        sp_r1 = load_spmat_by_id(r1)\n",
    "        sp_r2 = load_spmat_by_id(r2)\n",
    "        sp_rh = load_spmat_by_id(rh)\n",
    "        \n",
    "        # for the inverse relations\n",
    "        if r1 >= INV_NUM:\n",
    "            sp_r1 = sp_r1.T\n",
    "        if r2 >= INV_NUM:\n",
    "            sp_r2 = sp_r2.T\n",
    "        \n",
    "        sp_rule_head = sp_r1.dot(sp_r2)\n",
    "        sp_new_head = sp_rule_head - coo_matrix.multiply(sp_rule_head, sp_rh)\n",
    "        sp_new_head = sp_new_head.tocoo()\n",
    "        \n",
    "        new_triples = []\n",
    "        for i in range(len(sp_new_head.row)):\n",
    "            new_triples.append((sp_new_head.row[i], rh, sp_new_head.col[i]))\n",
    "            \n",
    "        return new_triples\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions about triples ###\n",
    "\n",
    "# get the set of (h, r) from triples\n",
    "def get_triples_hr_set(triples):\n",
    "    hr_set = set()\n",
    "    for triple in triples:\n",
    "        hr_set.add((triple[0], triple[1]))\n",
    "    return hr_set\n",
    "\n",
    "# get the set of relations from triples\n",
    "def get_triples_rel_set(triples):\n",
    "    rel_set = set()\n",
    "    for triple in triples:\n",
    "        rel_set.add(triple[1])\n",
    "    return rel_set\n",
    "\n",
    "# get rule list concerning relations in rel_set\n",
    "def rule_filter_rel(rules_lists, rel_set):\n",
    "    rules_filter = []\n",
    "    for i in range(len(rules_lists)):\n",
    "        if rules_lists[i][0][-1] in rel_set:\n",
    "            rules_filter.append(rules_lists[i])\n",
    "    return rules_filter\n",
    "\n",
    "# filter rules by their PCA_Confidence and Positive_Examples\n",
    "def rule_filter_PCA_POS(rules_lists, pca_conf=0, pos_num=0):\n",
    "    rules_filter = []\n",
    "    for i in range(len(rules_lists)):\n",
    "        if rules_lists[i][-1] >= pca_conf and rules_lists[i][1] >= pos_num:\n",
    "            rules_filter.append(rules_lists[i])\n",
    "    return rules_filter\n",
    "\n",
    "# filter rules by their PCA_Confidence\n",
    "def rule_filter_PCA_CONF(rules_lists, pca_conf_min=0, pca_conf_max=1.01):\n",
    "    rules_filter = []\n",
    "    for i in range(len(rules_lists)):\n",
    "        if rules_lists[i][-1] >= pca_conf_min and rules_lists[i][-1] < pca_conf_max:\n",
    "            rules_filter.append(rules_lists[i])\n",
    "    return rules_filter\n",
    "\n",
    "# get the dict of r to h\n",
    "def get_hr_set_dict(hr_set, rel_set):\n",
    "    hr_set_dict = {}\n",
    "    for rel in rel_set:\n",
    "        hr_set_dict[rel] = []\n",
    "    for hr in hr_set:\n",
    "        hr_set_dict[hr[1]].append(hr[0])\n",
    "    return hr_set_dict\n",
    "\n",
    "# filter the triples concerning relations in rel_set\n",
    "def filter_triples_by_relset(triples, rel_set):\n",
    "    filterd_triples = []\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[1] in rel_set:\n",
    "            filterd_triples.append(triple)\n",
    "    return filterd_triples\n",
    "\n",
    "# count the frequency of relations in the triples\n",
    "def count_reltri_num(triples):\n",
    "    rel_tri_num_dict = {}\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[1] in rel_tri_num_dict.keys():\n",
    "            rel_tri_num_dict[triple[1]] += 1\n",
    "        else:\n",
    "            rel_tri_num_dict[triple[1]] = 1\n",
    "    return rel_tri_num_dict\n",
    "\n",
    "# filter triples by relations\n",
    "def filter_triples_by_relset_del(triples, rel_set):\n",
    "    filterd_triples = []\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[1] not in rel_set:\n",
    "            filterd_triples.append(triple)\n",
    "    return filterd_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relation matrix #\n",
    "\n",
    "train_hrt = np.load(train_hrt_data_path) \n",
    "\n",
    "num_shape = 87143637 # num_entities\n",
    "def list_sparse_mat(r_list):\n",
    "    sp_mat = sparse.coo_matrix((np.ones(len(r_list[0])),(r_list[0],r_list[1])),shape=(num_shape, num_shape))\n",
    "    return sp_mat\n",
    "\n",
    "num_relations = 1315\n",
    "\n",
    "# get the head and tail entities of relations\n",
    "rel_ht_lists = []\n",
    "for rel in range(num_relations):\n",
    "    rel_ht_lists.append([[], []])\n",
    "    \n",
    "for i in tqdm(range(len(train_hrt))):\n",
    "    h = train_hrt[i][0]\n",
    "    r = train_hrt[i][1]\n",
    "    t = train_hrt[i][2]\n",
    "    \n",
    "    rel_ht_lists[r][0].append(h)\n",
    "    rel_ht_lists[r][1].append(t)\n",
    "    \n",
    "for rel in tqdm(range(num_relations)):\n",
    "    sp_mat_rel = list_sparse_mat(rel_ht_lists[rel])\n",
    "    sparse.save_npz(relation_matrix_path + 'rel_ht_spmat_'+str(rel)+'.npz', sp_mat_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the rules from raw rule files, and merge them\n",
    "rules_lists_all = []\n",
    "for data_path in data_path_list:\n",
    "    rules_lists_all += get_rules_from_file(data_path)\n",
    "    \n",
    "rules_dict_all = get_rule_PCA_examples(rules_lists_all)\n",
    "rules_lists_all_merge = get_rule_list_from_dict(rules_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rules_lists_all_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'None' value in the rule list\n",
    "# rules such as（x, x, x）（a, r, b） => (a, r, b) can get 'None' after parsing\n",
    "del_index = []\n",
    "len_tmp = len(rules_lists_all_merge)\n",
    "for i in range(len_tmp):\n",
    "    if rules_lists_all_merge[i][0] == None:\n",
    "       del_index.append(i)\n",
    "for i in range(len(del_index)):\n",
    "    rules_lists_all_merge.pop(del_index[i]-i)\n",
    "    \n",
    "rules_lists_all_merge = sorted(rules_lists_all_merge, key=lambda rule_list: rule_list[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rules_lists_all_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some dicts\n",
    "\n",
    "num_entities = 87143637\n",
    "\n",
    "test_hr = np.load('./ogb/wikikg90m_kddcup2021/processed/test_hr.npy').tolist()\n",
    "val_hr = np.load('./ogb/wikikg90m_kddcup2021/processed/val_hr.npy').tolist()\n",
    "test_val_hr = test_hr + val_hr\n",
    "\n",
    "test_val_hr_dict = {}\n",
    "\n",
    "for itm in test_val_hr:\n",
    "    if itm[1] not in test_val_hr_dict.keys():\n",
    "        test_val_hr_dict[itm[1]] = set()\n",
    "        test_val_hr_dict[itm[1]].add(itm[0])\n",
    "    else:\n",
    "        test_val_hr_dict[itm[1]].add(itm[0])\n",
    "\n",
    "        \n",
    "# get the set of entities\n",
    "test_val_ent_set = set()\n",
    "\n",
    "for itm in test_val_hr:\n",
    "    test_val_ent_set.add(itm[0])\n",
    "\n",
    "    \n",
    "# get the dict of entities\n",
    "ent_inValTest_dict = {}\n",
    "\n",
    "for ent in tqdm(range(num_entities)):\n",
    "    ent_inValTest_dict[ent] = 0\n",
    "for ent in tqdm(test_val_ent_set):\n",
    "    ent_inValTest_dict[ent] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rules with PCA Confidence > 0.95, the process to get the rules with PCA Confidence > 0.99 are similar.\n",
    "\n",
    "filterd_rules = rule_filter_PCA_CONF(rules_lists_all_merge, 0.95)\n",
    "filterd_rules_relDict = {}\n",
    "for rule_itm in filterd_rules:\n",
    "    rule_head = rule_itm[0][-1]\n",
    "    if rule_itm[0][-1] in filterd_rules_relDict.keys():\n",
    "        filterd_rules_relDict[rule_head].append(rule_itm[0])\n",
    "    else:\n",
    "        filterd_rules_relDict[rule_head] = [rule_itm[0]]\n",
    "        \n",
    "len_rel = []\n",
    "for key in filterd_rules_relDict.keys():\n",
    "    len_rel.append(len(filterd_rules_relDict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filterd_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions \n",
    "\n",
    "pca095_path_1 = './pool_files_pca095/'\n",
    "pca095_path_2 = './pool_files_pca095_filter/'\n",
    "\n",
    "def rule_task(rel_head, rule_set):\n",
    "    new_triples = []\n",
    "\n",
    "    for rule in rule_set:\n",
    "        new_triples += get_new_triples_by_rule_scipy(rule)\n",
    "    new_triples = list(set(new_triples))\n",
    "    \n",
    "    np.save(pca095_path_1+'pca095_rel_'+str(rel_head)+'_ruleNewTriples.npy', new_triples)\n",
    "    print(str(rel_head)+' DONE!')\n",
    "\n",
    "def multi_process_task(max_pool_num):\n",
    "    process_pool = Pool(max_pool_num)\n",
    "    for key in filterd_rules_relDict.keys():\n",
    "        process_pool.apply_async(rule_task, args=(key,filterd_rules_relDict[key]))\n",
    "    print('Wait the subprocesses ......')\n",
    "    process_pool.close()\n",
    "    process_pool.join()\n",
    "    print('All subprocesses done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_process_task(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep predictions about valid and test data\n",
    "\n",
    "def rule_task_inVALTEST_by_ent(rel_head):\n",
    "    print(str(rel_head)+' START!')\n",
    "    new_triples = []\n",
    "    valTest_new_triples = []\n",
    "    if os.path.exists(pca095_path_2+'pca095_rel_'+str(rel_head)+'_valTestENT_NewTriples.npy'):\n",
    "        print(str(rel_head)+' ALREADY DONE!')\n",
    "    else:\n",
    "        if os.path.exists(pca095_path_1+'pca095_rel_'+str(rel_head)+'_ruleNewTriples.npy'):\n",
    "            new_triples += np.load(pca095_path_1+'pca095_rel_'+str(rel_head)+'_ruleNewTriples.npy').astype(np.int32).tolist()\n",
    "\n",
    "        if new_triples != []:\n",
    "            valTest_new_triples = [ triple for triple in new_triples if ent_inValTest_dict[triple[2]] == 1 or ent_inValTest_dict[triple[0]] == 1 ]\n",
    "            np.save(pca095_path_2 + '/pca095_rel_'+str(rel_head)+'_valTestENT_NewTriples.npy', np.array(valTest_new_triples).astype(np.int32))\n",
    "\n",
    "            print(str(rel_head)+' DONE!')\n",
    "        else:\n",
    "            print(str(rel_head)+' NONE DONE!')\n",
    "\n",
    "            \n",
    "def multi_process_task_inVALTEST_ENT(max_pool_num):\n",
    "    process_pool = Pool(max_pool_num)\n",
    "    for rel in filterd_rules_relDict.keys():\n",
    "        process_pool.apply_async(rule_task_inVALTEST_by_ent, args=(rel,))\n",
    "    print('Wait the subprocesses ......')\n",
    "    process_pool.close()\n",
    "    process_pool.join()\n",
    "    print('All subprocesses done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_process_task_inVALTEST_ENT(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "all_new_triples = []\n",
    "for rel_head in tqdm(filterd_rules_relDict.keys()):\n",
    "    if os.path.exists(pca095_path_2+'pca095_rel_'+str(rel_head)+'_valTestENT_NewTriples.npy'):\n",
    "        all_new_triples += np.load(pca095_path_2+'pca095_rel_'+str(rel_head)+'_valTestENT_NewTriples.npy').tolist()\n",
    "\n",
    "train_hrt = np.load(train_hrt_data_path)\n",
    "np.save('./enhanced_triples/train_hrt_pca095_ENT.npy',\n",
    "        np.vstack((train_hrt, np.array(all_new_triples).astype(np.int32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
